{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associative retrieval task from [fast weights RNN](https://arxiv.org/abs/1610.06258):\n",
    ">  To solve this task, a standard RNN\n",
    "has to end up with hidden activities that somehow store all of the key-value pairs after the keys and\n",
    "values are presented sequentially. This makes it a significant challenge for models only using slow\n",
    "weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self):\n",
    "        alphabet = [chr(i) for i in range(ord(\"a\"), ord(\"z\") + 1)]\n",
    "        numbers = [str(i) for i in range(10)]\n",
    "        self.idx2word = alphabet + numbers + [\"?\"]\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.idx2word)}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self.idx2word[key]\n",
    "        elif isinstance(key, str):\n",
    "            return self.word2idx[key]\n",
    "        else:\n",
    "            raise TypeError\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssocRetrievalDataset(Dataset):\n",
    "    def __init__(self, filename: str, dictionary: Dictionary):\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "        data = [line.strip() for line in data]\n",
    "        inputs, targets = zip(*[line.split(\",\") for line in data])\n",
    "        inputs = [[dictionary[ch] for ch in inp] for inp in inputs]\n",
    "        targets = [dictionary[target] for target in targets]\n",
    "        self.inputs = torch.LongTensor(inputs)\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.inputs[idx]\n",
    "        target = self.targets[idx]\n",
    "        return inp, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(input_size + hidden_size, 4 * hidden_size)\n",
    "        )\n",
    "        self.bias = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "        self.ln = nn.LayerNorm(4 * hidden_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1 / math.sqrt(self.hidden_size)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, hx):\n",
    "        h, c = hx\n",
    "        xh = torch.cat([x, h], dim=1)\n",
    "        gates = self.ln(xh @ self.weight + self.bias)\n",
    "        gates = torch.chunk(gates, 4, dim=1)\n",
    "        f = torch.sigmoid(gates[0])\n",
    "        i = torch.sigmoid(gates[1])\n",
    "        o = torch.sigmoid(gates[2])\n",
    "        g = torch.tanh(gates[3])\n",
    "        c = f * c + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str = \"lstm\",\n",
    "        embed_size: int = 100,\n",
    "        num_cells: int = 50,\n",
    "        hidden_size: int = 100,\n",
    "        input_length: int = 8,\n",
    "        gap_length: int = 2,\n",
    "        data_path: str = os.getcwd(),\n",
    "        batch_size: int = 128,\n",
    "        num_workers: int = 4,\n",
    "        lr: float = 1e-3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_cells = num_cells\n",
    "        self.input_length = input_length\n",
    "        self.gap_length = gap_length\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.lr = lr\n",
    "\n",
    "        self.dictionary = Dictionary()\n",
    "        self.embed = nn.Embedding(len(self.dictionary), embed_size)\n",
    "        if model_type == \"lstm\":\n",
    "            self.rnn = nn.LSTMCell(embed_size, num_cells)\n",
    "        elif model_type == \"ln-lstm\":\n",
    "            self.rnn = LayerNormLSTMCell(embed_size, num_cells)\n",
    "        self.h_init = nn.Parameter(0.01 * torch.randn(1, num_cells))\n",
    "        self.c_init = nn.Parameter(0.01 * torch.randn(1, num_cells))\n",
    "        self.fc1 = nn.Linear(num_cells, hidden_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, len(self.dictionary))\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.loss = nn.NLLLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.h_init.expand(x.size(1), -1)\n",
    "        c = self.c_init.expand(x.size(1), -1)\n",
    "        for x_t in x:\n",
    "            x_t = self.embed(x_t)\n",
    "            h, c = self.lstm(x_t, (h, c))\n",
    "        y = self.relu(self.fc1(h))\n",
    "        y = self.log_softmax(self.fc2(y))\n",
    "        return y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        def generate_task():\n",
    "            input_abc = self.dictionary.idx2word[:-1]  # without \"?\"\n",
    "            inputs = random.sample(input_abc, k=self.input_length)\n",
    "            query_idx = random.randrange(self.input_length - 1)\n",
    "            query = inputs[query_idx]\n",
    "            target = inputs[query_idx + 1]\n",
    "            task = \"\".join(inputs + self.gap_length * [\"?\"] + [query])\n",
    "            return task, target\n",
    "\n",
    "        train_path = os.path.join(self.data_path, \"assoc_train.txt\")\n",
    "        with open(train_path, \"w\") as f:\n",
    "            for _ in range(100000):\n",
    "                task, target = generate_task()\n",
    "                f.write(f\"{task},{target}\\n\")\n",
    "\n",
    "        valid_path = os.path.join(self.data_path, \"assoc_valid.txt\")\n",
    "        with open(\"assoc_valid.txt\", \"w\") as f:\n",
    "            for _ in range(10000):\n",
    "                task, target = generate_task()\n",
    "                f.write(f\"{task},{target}\\n\")\n",
    "\n",
    "        test_path = os.path.join(self.data_path, \"assoc_test.txt\")\n",
    "        with open(\"assoc_test.txt\", \"w\") as f:\n",
    "            for _ in range(20000):\n",
    "                task, target = generate_task()\n",
    "                f.write(f\"{task},{target}\\n\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data_path = os.path.join(self.data_path, \"assoc_train.txt\")\n",
    "        train_set = AssocRetrievalDataset(data_path, self.dictionary)\n",
    "        train_loader = DataLoader(\n",
    "            train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        data_path = os.path.join(self.data_path, \"assoc_valid.txt\")\n",
    "        valid_set = AssocRetrievalDataset(data_path, self.dictionary)\n",
    "        valid_loader = DataLoader(\n",
    "            valid_set,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return valid_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        data_path = os.path.join(self.data_path, \"assoc_test.txt\")\n",
    "        test_set = AssocRetrievalDataset(data_path, self.dictionary)\n",
    "        test_loader = DataLoader(\n",
    "            test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return test_loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        output = self(data.T)\n",
    "        loss = self.loss(output, target)\n",
    "        return pl.TrainResult(loss)\n",
    "\n",
    "    def __eval_step(self, batch, batch_idx, prefix):\n",
    "        data, target = batch\n",
    "        output = self(data.T)\n",
    "        loss = self.loss(output, target)\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        acc = accuracy(preds, target)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log(f\"{prefix}_loss\", loss, prog_bar=True)\n",
    "        result.log(f\"{prefix}_acc\", acc, prog_bar=True)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.__eval_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.__eval_step(batch, batch_idx, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Model(model_type=\"mg-rnn\", lr=1e-3)\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "char-rnn",
   "language": "python",
   "name": "char-rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
